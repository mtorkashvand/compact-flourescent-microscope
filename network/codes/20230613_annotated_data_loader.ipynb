{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d36a47-49ec-4879-9c07-a510c3bd41bd",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a7a6da-1267-487e-87d6-dd13f4bd20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from h5py import File as h5File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e88d43-880e-48f0-8d28-f36f122f1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7161b-2b73-447e-b108-f79c9b27ce24",
   "metadata": {},
   "source": [
    "# GLOBAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3554dc3a-4b29-479e-a00b-764031c9d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "IMG_OUTPUT_DIM = 100\n",
    "\n",
    "IMAGING_TYPE = 'BACKGROUND_BRIGHT_WORM_DARK'\n",
    "CROP_SIZE = 400\n",
    "WINDOWSIZE_MIN_INCLUDED = 120\n",
    "WINDOWSIZE_MIN_INCLUDED2 = WINDOWSIZE_MIN_INCLUDED//2\n",
    "# THETA_MAX_DEVIATION = 360.0\n",
    "GAMMA_MAX_DEVIATION = 0.2\n",
    "# TODO: add scale changing which should change the bounds for x/y_min/max (sice size of included window should change)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf108b-607e-4ebf-bc8f-f4e3a16973ab",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b42dce-1624-4d1f-aaad-929ad9365e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_annotations_to_npz(fp_training_data):\n",
    "#     training_data = h5File(os.path.join(fp_training_data, 'training_data.h5'))\n",
    "#     data = training_data['data']\n",
    "#     nt, nx, ny = data.shape\n",
    "#     t_idx = np.arange(nt)\n",
    "#     coords = training_data['annotations'][:]\n",
    "#     np.savez_compressed(\n",
    "#         os.path.join(fp_training_data, 'labelled_data.npz'),\n",
    "#         images = data[:],\n",
    "#         ts = t_idxs,\n",
    "#         ys_idx = coords[:, 0],\n",
    "#         xs_idx = coords[:, 1],\n",
    "#         coords = coords,\n",
    "#     )\n",
    "#     training_data.close()\n",
    "#     return data[:], coords\n",
    "\n",
    "# def data_annotations_to_npz(fp_read_data, fp_read_annotations, fp_write_npz):\n",
    "#     file_data = h5File(fp_read_data)\n",
    "#     file_annotations = h5File(fp_read_annotations)\n",
    "    \n",
    "#     data = file_data['data']\n",
    "#     nt, nx, ny = data.shape\n",
    "\n",
    "#     t_idxs = file_annotations['t_idx'][:]\n",
    "#     xs = file_annotations['x'][:]\n",
    "#     ys = file_annotations['y'][:]\n",
    "    \n",
    "#     xs_idx = (xs*nx).astype(np.int64)\n",
    "#     ys_idx = (ys*ny).astype(np.int64)\n",
    "\n",
    "#     images = np.array([\n",
    "#         data[t] for t in t_idxs\n",
    "#     ])\n",
    "#     coords = np.stack((xs_idx, ys_idx)).T\n",
    "#     np.savez_compressed(\n",
    "#         fp_write_npz,\n",
    "#         images = images,\n",
    "#         ts = t_idxs,\n",
    "#         xs_idx = xs_idx, ys_idx = ys_idx,\n",
    "#         coords = coords,\n",
    "#     )\n",
    "#     file_data.close()\n",
    "#     file_annotations.close()\n",
    "#     return images, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ed805-1c3e-4c8e-adae-558811b16f6e",
   "metadata": {},
   "source": [
    "# Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6eca4d2-c70d-4cf0-aef1-3dbe539ba3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_xy(M, x, y):\n",
    "    return np.matmul(\n",
    "        M,\n",
    "        np.array([x, y, 1])\n",
    "    )\n",
    "\n",
    "class AnnotatedDataLoader(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, fp_annotated_data, factor_augmentations = 1):\n",
    "        super().__init__()\n",
    "        self.fp_annotated_data = fp_annotated_data\n",
    "        self._data_file = h5File(self.fp_annotated_data)\n",
    "        self.images = self._data_file['data']\n",
    "        self.nrecords, self.ny, self.nx = self.images.shape\n",
    "        self.coordinates = self._data_file['annotations']\n",
    "        self.factor_augmentations = factor_augmentations\n",
    "        self.n = self.nrecords * self.factor_augmentations\n",
    "        return\n",
    "    # Length\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    # Get Item\n",
    "    def __getitem__(self, i):\n",
    "        # Parameters\n",
    "        idx = i // self.factor_augmentations\n",
    "        # Choose randomly from 4 possible rotations: 0, 90, 180, 270 degrees\n",
    "        theta = np.random.choice([ 0, 90, 180, 270 ])\n",
    "        nx, ny = self.nx, self.ny\n",
    "        img_processed = self.images[idx]\n",
    "        y_idx, x_idx = self.coordinates[idx]\n",
    "        label = 1\n",
    "        if y_idx >= ny or x_idx >= nx:\n",
    "            label = 0\n",
    "            y_idx, x_idx = ny//2, nx//2\n",
    "        # Gamma\n",
    "        gamma = 1.0 + (np.random.rand()-0.5)*2*GAMMA_MAX_DEVIATION\n",
    "        img_processed = ( (img_processed/255)**gamma * 255 ).astype(np.uint8)\n",
    "        \n",
    "        M_rotation = cv.getRotationMatrix2D((nx//2, ny//2), theta, 1.0)\n",
    "        img_processed = cv.warpAffine(\n",
    "            img_processed,\n",
    "            M_rotation,\n",
    "            (self.nx, self.ny),\n",
    "            borderValue=255\n",
    "        )\n",
    "        coords_new = rotate_xy(M_rotation, x_idx, y_idx).astype(np.int32)\n",
    "        \n",
    "        # If Annotation is close to the edge, discard the \"Minimum Window Criteria\"\n",
    "        # windowsize_min_included2 = WINDOWSIZE_MIN_INCLUDED2\n",
    "        # if np.any(coords_new-windowsize_min_included2 < 0) or np.any(coords_new+windowsize_min_included2 >= np.array([nx, ny])):\n",
    "        #     windowsize_min_included2 = 0\n",
    "        #     if DEBUG:\n",
    "        #         print(f\"Annotation close to edge encountered! Image IDX: {idx} Label: {label}, Coords: {coords_new}\")\n",
    "        \n",
    "        # TODO add passing conditions and handle missing annotation inside the frame\n",
    "        for _ in range(3):\n",
    "            # Annotation Outside after Rotation\n",
    "            if np.any(coords_new >= np.array([nx, ny])) or np.any(coords_new < np.zeros(2)):\n",
    "                if DEBUG:\n",
    "                    print(\"Rotation@annotation outside image: {}-{} , shape:({},{})\".format(\n",
    "                        *coords_new,\n",
    "                        nx, ny\n",
    "                    ))\n",
    "                continue\n",
    "            if DEBUG:\n",
    "                print(f\"Image IDX: {idx} Label: {label}, COORD NEW: {coords_new}\")\n",
    "            x_idx_new, y_idx_new = coords_new\n",
    "            # x_idx_min = max(\n",
    "            #     x_idx_new + windowsize_min_included2 - CROP_SIZE,\n",
    "            #     0\n",
    "            # )\n",
    "            # x_idx_max = min(\n",
    "            #     x_idx_new - windowsize_min_included2,\n",
    "            #     nx - CROP_SIZE\n",
    "            # )\n",
    "            # y_idx_min = max(\n",
    "            #     y_idx_new + windowsize_min_included2 - CROP_SIZE,\n",
    "            #     0\n",
    "            # )\n",
    "            # y_idx_max = min(\n",
    "            #     y_idx_new - windowsize_min_included2,\n",
    "            #     ny - CROP_SIZE\n",
    "            # )\n",
    "            # If Conditions\n",
    "            ## X\n",
    "            if x_idx_new <= WINDOWSIZE_MIN_INCLUDED2:\n",
    "                x_idx_min, x_idx_max = 0, 0\n",
    "            elif x_idx_new >= (nx - WINDOWSIZE_MIN_INCLUDED2):\n",
    "                x_idx_min, x_idx_max = nx-CROP_SIZE, nx-CROP_SIZE\n",
    "            elif x_idx_new <= nx//2:\n",
    "                x_idx_min, x_idx_max = 0, min(x_idx_new - WINDOWSIZE_MIN_INCLUDED2, nx-CROP_SIZE)\n",
    "            elif x_idx_new > nx//2:\n",
    "                x_idx_min, x_idx_max = max(0, x_idx_new+WINDOWSIZE_MIN_INCLUDED2-CROP_SIZE), nx-CROP_SIZE\n",
    "            else:\n",
    "                print(\"HOY!!!!! X\")\n",
    "            ## Y\n",
    "            if y_idx_new <= WINDOWSIZE_MIN_INCLUDED2:\n",
    "                y_idx_min, y_idx_max = 0, 0\n",
    "            elif y_idx_new >= (ny - WINDOWSIZE_MIN_INCLUDED2):\n",
    "                y_idx_min, y_idx_max = ny-CROP_SIZE, ny-CROP_SIZE\n",
    "            elif y_idx_new <= ny//2:\n",
    "                y_idx_min, y_idx_max = 0, min(y_idx_new - WINDOWSIZE_MIN_INCLUDED2, ny-CROP_SIZE)\n",
    "            elif y_idx_new > ny//2:\n",
    "                y_idx_min, y_idx_max = max(0, y_idx_new+WINDOWSIZE_MIN_INCLUDED2-CROP_SIZE), ny-CROP_SIZE\n",
    "            else:\n",
    "                print(\"HOY!!!!! Y\")\n",
    "            # DEBUG\n",
    "            if DEBUG:\n",
    "                print(\"IDX X MIN-MAX: {}-{} , Y MIN-MAX: {}-{}\".format(\n",
    "                    x_idx_min, x_idx_max,\n",
    "                    y_idx_min, y_idx_max\n",
    "                ))\n",
    "            # Empty Cropping Area\n",
    "            if x_idx_max < x_idx_min or y_idx_max < y_idx_min:\n",
    "                if DEBUG:\n",
    "                    print(\"Empty crop area: {}-{} , {}-{}\".format(\n",
    "                        x_idx_min, x_idx_max,\n",
    "                        y_idx_min, y_idx_max\n",
    "                    ))\n",
    "                continue\n",
    "            if DEBUG:\n",
    "                print(f\"{x_idx_min}-{x_idx_max} , {y_idx_min}-{y_idx_max}\")\n",
    "            crop_topleft = np.random.randint(\n",
    "                (y_idx_min, x_idx_min),\n",
    "                (y_idx_max+1,x_idx_max+1)\n",
    "            )  # so `x` can be used for index `i` in slicing and `y` for column indexing\n",
    "            if DEBUG:\n",
    "                print(f\"{crop_topleft}\")\n",
    "            # Apply\n",
    "            imin, jmin = crop_topleft\n",
    "            imax, jmax = crop_topleft+CROP_SIZE\n",
    "            img_processed = img_processed[imin:imax, jmin:jmax]\n",
    "            coords_new -= crop_topleft[::-1]\n",
    "            # Return\n",
    "            if DEBUG:\n",
    "                print(f\"Image IDX: {idx} Label: {label}, COORD NEW Cropped: {coords_new}\")\n",
    "            return img_processed, coords_new, label\n",
    "        print(\"DEBUG: Attempts to Augment failed!\")\n",
    "        print(f\"Image IDX: {idx} Label: {label}, Coords: {coords_new}\")\n",
    "        raise NotImplementedError()\n",
    "        # return self.images[idx], self.coordinates[idx]\n",
    "\n",
    "# def rotate_xy(M, x_idx, y_idx):\n",
    "#     return np.matmul(\n",
    "#         M,\n",
    "#         np.array([x_idx, y_idx, 1])\n",
    "#     )\n",
    "\n",
    "# class AnnotatedDataLoader(Dataset):\n",
    "#     # Constructor\n",
    "#     def __init__(self, images, coordinates, factor_augmentations = 1):\n",
    "#         super().__init__()\n",
    "#         self.images = images\n",
    "#         self.nrecords, self.nx, self.ny = self.images.shape\n",
    "#         self.coordinates = coordinates\n",
    "#         self.factor_augmentations = factor_augmentations\n",
    "#         self.n = self.nrecords * self.factor_augmentations\n",
    "#         return\n",
    "#     # Length\n",
    "#     def __len__(self):\n",
    "#         return self.n\n",
    "#     # Get Item\n",
    "#     def __getitem__(self, i):\n",
    "#         if self.factor_augmentations == 1:\n",
    "#             return self.images[i].copy(), self.coordinates[i].copy()\n",
    "#         # Sample\n",
    "#         nx, ny = self.nx, self.ny\n",
    "#         idx = i//self.factor_augmentations\n",
    "#         img_processed = self.images[idx]\n",
    "#         x_idx, y_idx = self.coordinates[idx]\n",
    "#         # Gamma\n",
    "#         gamma = 1.0 + (np.random.rand()-0.5)*2*GAMMA_MAX_DEVIATION\n",
    "#         img_processed = ( (img_processed/255)**gamma * 255 ).astype(np.uint8)\n",
    "#         # TODO add passing conditions and handle missing annotation inside the frame\n",
    "#         for _ in range(3):\n",
    "#             # Rotation\n",
    "#             theta = np.random.rand()*THETA_MAX_DEVIATION\n",
    "#             M_rotation = cv.getRotationMatrix2D((nx//2, ny//2), theta, 1.0)\n",
    "#             assert IMAGING_TYPE == 'BACKGROUND_BRIGHT_WORM_DARK', \\\n",
    "#                 \"`borderValue` for rotation is set only for dark worm on bright background.\"\n",
    "#             img_processed = cv.warpAffine(\n",
    "#                 img_processed,\n",
    "#                 M_rotation,\n",
    "#                 (self.nx, self.ny),\n",
    "#                 borderValue=255\n",
    "#             )\n",
    "#             # Offset\n",
    "#             coords_new = rotate_xy(M_rotation, x_idx, y_idx).astype(np.int32)\n",
    "#             # Annotation Outside after Rotation\n",
    "#             if np.any(coords_new >= np.array([nx, ny])) or np.any(coords_new < np.zeros(2)):\n",
    "#                 if DEBUG:\n",
    "#                     print(\"Rotation@annotation outside image: {}-{} , shape:({},{})\".format(\n",
    "#                         *coords_new,\n",
    "#                         nx, ny\n",
    "#                     ))\n",
    "#                 continue\n",
    "#             if DEBUG:\n",
    "#                 print(f\"COORD NEW: {coords_new}\")\n",
    "#             x_idx_new, y_idx_new = coords_new\n",
    "#             x_idx_min = max(\n",
    "#                 x_idx_new + WINDOWSIZE_MIN_INCLUDED2 - CROP_SIZE,\n",
    "#                 0\n",
    "#             )\n",
    "#             x_idx_max = min(\n",
    "#                 x_idx_new - WINDOWSIZE_MIN_INCLUDED2,\n",
    "#                 nx - CROP_SIZE\n",
    "#             )\n",
    "#             y_idx_min = max(\n",
    "#                 y_idx_new + WINDOWSIZE_MIN_INCLUDED2 - CROP_SIZE,\n",
    "#                 0\n",
    "#             )\n",
    "#             y_idx_max = min(\n",
    "#                 y_idx_new - WINDOWSIZE_MIN_INCLUDED2,\n",
    "#                 ny - CROP_SIZE\n",
    "#             )\n",
    "#             # Empty Cropping Area\n",
    "#             if x_idx_max < x_idx_min or y_idx_max < y_idx_min:\n",
    "#                 if DEBUG:\n",
    "#                     print(\"Empty crop area: {}-{} , {}-{}\".format(\n",
    "#                         x_idx_min, x_idx_max,\n",
    "#                         y_idx_min, y_idx_max\n",
    "#                     ))\n",
    "#                 continue\n",
    "#             if DEBUG:\n",
    "#                 print(f\"{x_idx_min}-{x_idx_max} , {y_idx_min}-{y_idx_max}\")\n",
    "#             crop_topleft = np.random.randint(\n",
    "#                 (x_idx_min, y_idx_min),\n",
    "#                 (x_idx_max+1,y_idx_max+1)\n",
    "#             )  # so `x` can be used for index `i` in slicing and `y` for column indexing\n",
    "#             if DEBUG:\n",
    "#                 print(f\"{crop_topleft}\")\n",
    "#             # Apply\n",
    "#             imin, jmin = crop_topleft\n",
    "#             imax, jmax = crop_topleft+CROP_SIZE\n",
    "#             img_processed = img_processed[imin:imax, jmin:jmax]\n",
    "#             coords_new -= crop_topleft[::-1]\n",
    "#             # Return\n",
    "#             return img_processed, coords_new\n",
    "#         print(\"DEBUG: Attempts to Augment failed!\")\n",
    "#         return self.images[idx].copy(), self.coordinates[idx].copy()\n",
    "def collate_fn_3d_input(data):\n",
    "    images, coords, _ = zip(*data)\n",
    "    coords = np.array(coords)\n",
    "    images = np.repeat(\n",
    "        np.array(images)[:,None,:,:], 3, axis=1\n",
    "    )\n",
    "    images_channeled = torch.tensor( images, dtype=torch.float32 )\n",
    "    coords = torch.tensor( coords, dtype=torch.float32 )\n",
    "    return images_channeled, coords\n",
    "def collate_fn_heatmap(data):\n",
    "    images, coords, labels = zip(*data)\n",
    "    _img = images[0]\n",
    "    images = np.array(images)[:,None,:,:]\n",
    "    images = torch.tensor( images, dtype=torch.float32 )\n",
    "    heatmaps = []\n",
    "    for (i,j), label in zip(coords,labels):\n",
    "        if label != 0:\n",
    "            img_annotated = np.zeros_like( _img, dtype=np.float32 )\n",
    "            img_annotated[j,i] = 100.0\n",
    "            img_annotated = cv.GaussianBlur(img_annotated,(11,11), 0)\n",
    "            img_annotated = cv.resize(img_annotated, (IMG_OUTPUT_DIM, IMG_OUTPUT_DIM))\n",
    "            img_annotated /= img_annotated.max()\n",
    "        else:\n",
    "            img_annotated = np.zeros( (IMG_OUTPUT_DIM, IMG_OUTPUT_DIM), dtype=np.float32 )\n",
    "        heatmaps.append(img_annotated.flatten())\n",
    "    heatmaps = torch.tensor( np.array(heatmaps), dtype=torch.float32 )\n",
    "    return images, heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef3361-4e96-4f8b-8a8a-04cd8e272573",
   "metadata": {},
   "source": [
    "# Convert Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ddd59-54fa-4f80-8857-d46ed6106c4a",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "423aa481-e0b6-4c71-8141-3b7bcca66982",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnnotatedDataLoader(\n",
    "    fp_annotated_data=r\"V:\\Mahdi\\OpenAutoScope2.0\\data\\training_data\\training_data.h5\",\n",
    "    factor_augmentations = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2799d28d-ba4b-430b-9c11-0f65f7efeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32, shuffle=True,\n",
    "    collate_fn=collate_fn_heatmap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2091f-545d-4791-9b37-395e4f3fbf9a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84736de-a72c-4209-8d29-02224abfd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_image_shape = (400, 400)):\n",
    "        super().__init__()\n",
    "        self.input_image_shape = input_image_shape\n",
    "        self.input_nx, self.input_ny = self.input_image_shape\n",
    "        # Convolutions\n",
    "        self.conv1_nchannels = 1\n",
    "        self.conv1_nconvs = 4\n",
    "        self.conv1_convsize = 25\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            self.conv1_nchannels,\n",
    "            self.conv1_nconvs,\n",
    "            self.conv1_convsize\n",
    "        )\n",
    "        self.conv1_activation = nn.ReLU()\n",
    "        self.conv1_npooling = 5\n",
    "        self.conv1_poolingstride = 2\n",
    "        self.conv1_pooling = nn.MaxPool2d(\n",
    "            self.conv1_npooling,\n",
    "            stride=self.conv1_poolingstride\n",
    "        )\n",
    "        # TODO: add max_pooling layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n",
    "        # Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Denses\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_features=138384,  # TODO calculate this based on parameters above, e.g. self.conv1_convsize, ...\n",
    "            out_features=64\n",
    "        )\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=64,\n",
    "            out_features=IMG_OUTPUT_DIM**2\n",
    "        )\n",
    "        self.to_probability = nn.Sigmoid()\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutions\n",
    "        x = self.conv1_activation(self.conv1(x))\n",
    "        x = self.conv1_pooling(x)\n",
    "        # Flattern\n",
    "        x = self.flatten(x)\n",
    "        # Dense\n",
    "        x = self.linear1_activation(\n",
    "            self.linear1(x)\n",
    "        )\n",
    "        x = self.dense(x)\n",
    "        return self.to_probability(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80cd80-5a99-4456-bd7c-6be6d9a04d5a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4bbc7be-bd13-4e58-a904-a629737662c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4375676-88bd-4bb3-9890-ed354b81b993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:   0.000:   0%|                                                                             | 0/1 [00:00<?, ?it/s]\n",
      "Epoch Steps - Loss:   0.000:   0%|                                                             | 0/497 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   0%|                                                     | 1/497 [00:00<05:48,  1.42it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   0%|▏                                                    | 2/497 [00:01<04:37,  1.78it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   1%|▎                                                    | 3/497 [00:01<04:13,  1.95it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   1%|▍                                                    | 4/497 [00:02<04:04,  2.02it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   1%|▌                                                    | 5/497 [00:02<04:00,  2.04it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   1%|▋                                                    | 6/497 [00:03<03:57,  2.07it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   1%|▋                                                    | 7/497 [00:03<03:56,  2.07it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   2%|▊                                                    | 8/497 [00:03<03:54,  2.09it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   2%|▉                                                    | 9/497 [00:04<03:51,  2.11it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   2%|█                                                   | 10/497 [00:04<03:50,  2.12it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   2%|█▏                                                  | 11/497 [00:05<03:49,  2.12it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   2%|█▎                                                  | 12/497 [00:05<03:45,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   3%|█▎                                                  | 13/497 [00:06<03:47,  2.12it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   3%|█▍                                                  | 14/497 [00:06<03:45,  2.14it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   3%|█▌                                                  | 15/497 [00:07<03:44,  2.14it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   3%|█▋                                                  | 16/497 [00:07<03:47,  2.12it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   3%|█▊                                                  | 17/497 [00:08<03:47,  2.11it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   4%|█▉                                                  | 18/497 [00:08<03:44,  2.13it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   4%|█▉                                                  | 19/497 [00:09<03:42,  2.14it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   4%|██                                                  | 20/497 [00:09<03:40,  2.16it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   4%|██▏                                                 | 21/497 [00:10<03:41,  2.14it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   4%|██▎                                                 | 22/497 [00:10<03:41,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   5%|██▍                                                 | 23/497 [00:10<03:40,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   5%|██▌                                                 | 24/497 [00:11<03:37,  2.17it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   5%|██▌                                                 | 25/497 [00:11<03:39,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   5%|██▋                                                 | 26/497 [00:12<03:40,  2.13it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   5%|██▊                                                 | 27/497 [00:12<03:42,  2.11it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   6%|██▉                                                 | 28/497 [00:13<03:40,  2.13it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   6%|███                                                 | 29/497 [00:13<03:37,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   6%|███▏                                                | 30/497 [00:14<03:37,  2.15it/s]\u001b[A\n",
      "Epoch Steps - Loss:   0.000:   6%|███▏                                                | 31/497 [00:14<03:34,  2.18it/s]\u001b[A\n",
      "Loss:   0.000:   0%|                                                                             | 0/1 [00:15<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_train_pred, y_train)\n\u001b[0;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 17\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = tqdm( range(1), desc=f'Loss: {0.0:>7.3f}', position=0 )\n",
    "for i_epoch in epochs:\n",
    "    losses_epoch = []\n",
    "    steps = tqdm(dataloader, desc=f'Epoch Steps - Loss: {0.0:>7.3f}',position=1, leave=False)\n",
    "    for x_train, y_train in steps:\n",
    "        x_train = x_train.to(DEVICE)\n",
    "        y_train = y_train.to(DEVICE)\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "        loss.backward()\n",
    "        loss_value = loss.cpu().item()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        losses_epoch.append(loss_value)\n",
    "        steps.set_description(\n",
    "            'Epoch Steps - Loss: {:>7.3f}'.format(loss_value)\n",
    "        )\n",
    "    logs.append([\n",
    "        np.mean(losses_epoch),\n",
    "        losses_epoch.copy()\n",
    "    ])\n",
    "    # Report\n",
    "    epochs.set_description(\n",
    "        'Loss: {:>7.3f}'.format( logs[-1][0] )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e666891-6b19-4686-88cd-bd660bdbe16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328cd8f-bef4-454f-bff4-81ff58bec188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020a3de-a545-4a64-93ac-5512b0f1b437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c169b-3993-4f62-a4d0-ad311abb644f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efad11-631a-477b-a9eb-a34188425ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc31c8-3f4a-4d9c-9c52-39d9be54dc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768bd26d-3768-435c-b73f-dcf184280dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5b5be-169f-4a68-905a-2f7eb0bfa045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90076d8-2da8-48b1-8c82-80aff1abbcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76e7f4-d4e2-4f75-99fc-34a019d1be13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96add95-41bc-4df3-a761-e82ee5e89f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf10eb-9755-4577-a399-bb7f25a0a698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5aeb0-b925-4e8f-b799-8ca6c7c5de2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aafb11-572e-4aea-b884-5cc7a578d861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840a58e-7f62-498b-a619-cc55ae6cbc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d6543-ba4b-400b-9ae6-35f4d5aeeba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7b24e-7933-4b40-9b00-434f7fbacded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0d02b-4fb7-430e-af6b-c7fe351c8287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5455389-e3fc-4ff0-87fc-ba68878be32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651166a-424a-4cd2-b50a-1970997ea6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6b73e-cbcd-488d-8a31-d27bc10c4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed6708-010c-4928-bd35-4c5974c0d34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413e49a-35f7-45ce-842a-1fbc7e25c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82f273-2efb-4d5f-b4ae-0fccde3e7574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd896c63-3a36-489b-b8c1-4a9fa9fa35fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
