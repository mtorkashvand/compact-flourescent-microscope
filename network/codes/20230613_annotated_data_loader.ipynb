{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d36a47-49ec-4879-9c07-a510c3bd41bd",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1a7a6da-1267-487e-87d6-dd13f4bd20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from h5py import File as h5File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4e88d43-880e-48f0-8d28-f36f122f1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7161b-2b73-447e-b108-f79c9b27ce24",
   "metadata": {},
   "source": [
    "# GLOBAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3554dc3a-4b29-479e-a00b-764031c9d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "IMG_OUTPUT_DIM = 128\n",
    "\n",
    "IMAGING_TYPE = 'BACKGROUND_BRIGHT_WORM_DARK'\n",
    "CROP_SIZE = 400\n",
    "WINDOWSIZE_MIN_INCLUDED = 120\n",
    "WINDOWSIZE_MIN_INCLUDED2 = WINDOWSIZE_MIN_INCLUDED//2\n",
    "THETA_MAX_DEVIATION = 360.0\n",
    "GAMMA_MAX_DEVIATION = 0.2\n",
    "# TODO: add scale changing which should change the bounds for x/y_min/max (sice size of included window should change)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf108b-607e-4ebf-bc8f-f4e3a16973ab",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68b42dce-1624-4d1f-aaad-929ad9365e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_annotations_to_npz(fp_read_data, fp_read_annotations, fp_write_npz):\n",
    "    file_data = h5File(fp_read_data)\n",
    "    file_annotations = h5File(fp_read_annotations)\n",
    "    \n",
    "    data = file_data['data']\n",
    "    nt, nx, ny = data.shape\n",
    "\n",
    "    t_idxs = file_annotations['t_idx'][:]\n",
    "    xs = file_annotations['x'][:]\n",
    "    ys = file_annotations['y'][:]\n",
    "    \n",
    "    xs_idx = (xs*nx).astype(np.int64)\n",
    "    ys_idx = (ys*ny).astype(np.int64)\n",
    "\n",
    "    images = np.array([\n",
    "        data[t] for t in t_idxs\n",
    "    ])\n",
    "    coords = np.stack((xs_idx, ys_idx)).T\n",
    "    np.savez_compressed(\n",
    "        fp_write_npz,\n",
    "        images = images,\n",
    "        ts = t_idxs,\n",
    "        xs_idx = xs_idx, ys_idx = ys_idx,\n",
    "        coords = coords,\n",
    "    )\n",
    "    file_data.close()\n",
    "    file_annotations.close()\n",
    "    return images, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ed805-1c3e-4c8e-adae-558811b16f6e",
   "metadata": {},
   "source": [
    "# Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c6eca4d2-c70d-4cf0-aef1-3dbe539ba3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_xy(M, x_idx, y_idx):\n",
    "    return np.matmul(\n",
    "        M,\n",
    "        np.array([x_idx, y_idx, 1])\n",
    "    )\n",
    "\n",
    "class AnnotatedDataLoader(Dataset):\n",
    "    # Constructor\n",
    "    def __init__(self, images, coordinates, factor_augmentations = 1):\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.nrecords, self.nx, self.ny = self.images.shape\n",
    "        self.coordinates = coordinates\n",
    "        self.factor_augmentations = factor_augmentations\n",
    "        self.n = self.nrecords * self.factor_augmentations\n",
    "        return\n",
    "    # Length\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    # Get Item\n",
    "    def __getitem__(self, i):\n",
    "        if self.factor_augmentations == 1:\n",
    "            return self.images[i].copy(), self.coordinates[i].copy()\n",
    "        # Sample\n",
    "        nx, ny = self.nx, self.ny\n",
    "        idx = i//self.factor_augmentations\n",
    "        img_processed = self.images[idx]\n",
    "        x_idx, y_idx = self.coordinates[idx]\n",
    "        # Gamma\n",
    "        gamma = 1.0 + (np.random.rand()-0.5)*2*GAMMA_MAX_DEVIATION\n",
    "        img_processed = ( (img_processed/255)**gamma * 255 ).astype(np.uint8)\n",
    "        # TODO add passing conditions and handle missing annotation inside the frame\n",
    "        for _ in range(3):\n",
    "            # Rotation\n",
    "            theta = np.random.rand()*THETA_MAX_DEVIATION\n",
    "            M_rotation = cv.getRotationMatrix2D((nx//2, ny//2), theta, 1.0)\n",
    "            assert IMAGING_TYPE == 'BACKGROUND_BRIGHT_WORM_DARK', \\\n",
    "                \"`borderValue` for rotation is set only for dark worm on bright background.\"\n",
    "            img_processed = cv.warpAffine(\n",
    "                img_processed,\n",
    "                M_rotation,\n",
    "                (self.nx, self.ny),\n",
    "                borderValue=255\n",
    "            )\n",
    "            # Offset\n",
    "            coords_new = rotate_xy(M_rotation, x_idx, y_idx).astype(np.int32)\n",
    "            # Annotation Outside after Rotation\n",
    "            if np.any(coords_new >= np.array([nx, ny])) or np.any(coords_new < np.zeros(2)):\n",
    "                if DEBUG:\n",
    "                    print(\"Rotation@annotation outside image: {}-{} , shape:({},{})\".format(\n",
    "                        *coords_new,\n",
    "                        nx, ny\n",
    "                    ))\n",
    "                continue\n",
    "            if DEBUG:\n",
    "                print(f\"COORD NEW: {coords_new}\")\n",
    "            x_idx_new, y_idx_new = coords_new\n",
    "            x_idx_min = max(\n",
    "                x_idx_new + WINDOWSIZE_MIN_INCLUDED2 - CROP_SIZE,\n",
    "                0\n",
    "            )\n",
    "            x_idx_max = min(\n",
    "                x_idx_new - WINDOWSIZE_MIN_INCLUDED2,\n",
    "                nx - CROP_SIZE\n",
    "            )\n",
    "            y_idx_min = max(\n",
    "                y_idx_new + WINDOWSIZE_MIN_INCLUDED2 - CROP_SIZE,\n",
    "                0\n",
    "            )\n",
    "            y_idx_max = min(\n",
    "                y_idx_new - WINDOWSIZE_MIN_INCLUDED2,\n",
    "                ny - CROP_SIZE\n",
    "            )\n",
    "            # Empty Cropping Area\n",
    "            if x_idx_max < x_idx_min or y_idx_max < y_idx_min:\n",
    "                if DEBUG:\n",
    "                    print(\"Empty crop area: {}-{} , {}-{}\".format(\n",
    "                        x_idx_min, x_idx_max,\n",
    "                        y_idx_min, y_idx_max\n",
    "                    ))\n",
    "                continue\n",
    "            if DEBUG:\n",
    "                print(f\"{x_idx_min}-{x_idx_max} , {y_idx_min}-{y_idx_max}\")\n",
    "            crop_topleft = np.random.randint(\n",
    "                (x_idx_min, y_idx_min),\n",
    "                (x_idx_max+1,y_idx_max+1)\n",
    "            )  # so `x` can be used for index `i` in slicing and `y` for column indexing\n",
    "            if DEBUG:\n",
    "                print(f\"{crop_topleft}\")\n",
    "            # Apply\n",
    "            imin, jmin = crop_topleft\n",
    "            imax, jmax = crop_topleft+CROP_SIZE\n",
    "            img_processed = img_processed[imin:imax, jmin:jmax]\n",
    "            coords_new -= crop_topleft[::-1]\n",
    "            # Return\n",
    "            return img_processed, coords_new\n",
    "        print(\"DEBUG: Attempts to Augment failed!\")\n",
    "        return self.images[idx].copy(), self.coordinates[idx].copy()\n",
    "def collate_fn_3d_input(data):\n",
    "    images, coords = zip(*data)\n",
    "    coords = np.array(coords)\n",
    "    images = np.repeat(\n",
    "        np.array(images)[:,None,:,:], 3, axis=1\n",
    "    )\n",
    "    images_channeled = torch.tensor( images, dtype=torch.float32 )\n",
    "    coords = torch.tensor( coords, dtype=torch.float32 )\n",
    "    return images_channeled, coords\n",
    "def collate_fn_heatmap(data):\n",
    "    images, coords = zip(*data)\n",
    "    _img = images[0]\n",
    "    images = np.array(images)[:,None,:,:]\n",
    "    images = torch.tensor( images, dtype=torch.float32 )\n",
    "    heatmaps = []\n",
    "    for j,i in coords:\n",
    "        img_annotated = np.zeros_like( _img, dtype=np.float32 )\n",
    "        img_annotated[i,j] = 100.0\n",
    "        img_annotated = cv.GaussianBlur(img_annotated,(11,11), 0)\n",
    "        img_annotated = cv.resize(img_annotated, (IMG_OUTPUT_DIM, IMG_OUTPUT_DIM))\n",
    "        img_annotated /= img_annotated.max()\n",
    "        heatmaps.append(img_annotated.flatten())\n",
    "    heatmaps = torch.tensor( heatmaps, dtype=torch.float32 )\n",
    "    return images, heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef3361-4e96-4f8b-8a8a-04cd8e272573",
   "metadata": {},
   "source": [
    "# Convert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2ed291a1-8bc9-4080-9cc2-9ca2dda7e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_data = \"../data/20230414_N2_pharyngeal_and_pumping_000003.h5\"\n",
    "fp_annotations = \"../data/20230414_N2_pharyngeal_and_pumping_000003_annotations.h5\"\n",
    "fp_labelled_data = \"../data/labelled_data.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d591c6b6-8bec-4959-88d5-f587d554e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(fp_labelled_data):\n",
    "    with np.load(fp_labelled_data) as in_file:\n",
    "        images = in_file['images']\n",
    "        coords = in_file['coords']\n",
    "else:\n",
    "    images, coords = data_annotations_to_npz(fp_data, fp_annotations, fp_labelled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ddd59-54fa-4f80-8857-d46ed6106c4a",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "423aa481-e0b6-4c71-8141-3b7bcca66982",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AnnotatedDataLoader(\n",
    "    images, coords,\n",
    "    factor_augmentations = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2799d28d-ba4b-430b-9c11-0f65f7efeb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32, shuffle=True,\n",
    "    collate_fn=collate_fn_heatmap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2091f-545d-4791-9b37-395e4f3fbf9a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b84736de-a72c-4209-8d29-02224abfd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_image_shape = (400, 400)):\n",
    "        super().__init__()\n",
    "        self.input_image_shape = input_image_shape\n",
    "        self.input_nx, self.input_ny = self.input_image_shape\n",
    "        # Convolutions\n",
    "        self.conv1_nchannels = 1\n",
    "        self.conv1_nconvs = 4\n",
    "        self.conv1_convsize = 25\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            self.conv1_nchannels,\n",
    "            self.conv1_nconvs,\n",
    "            self.conv1_convsize\n",
    "        )\n",
    "        self.conv1_activation = nn.ReLU()\n",
    "        self.conv1_npooling = 5\n",
    "        self.conv1_poolingstride = 2\n",
    "        self.conv1_pooling = nn.MaxPool2d(\n",
    "            self.conv1_npooling,\n",
    "            stride=self.conv1_poolingstride\n",
    "        )\n",
    "        # TODO: add max_pooling layers\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n",
    "        # Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Denses\n",
    "        self.linear1 = nn.Linear(\n",
    "            in_features=234256,  # TODO calculate this based on parameters above, e.g. self.conv1_convsize, ...\n",
    "            out_features=64\n",
    "        )\n",
    "        self.linear1_activation = nn.ReLU()\n",
    "        self.dense = nn.Linear(\n",
    "            in_features=64,\n",
    "            out_features=IMG_OUTPUT_DIM**2\n",
    "        )\n",
    "        self.to_probability = nn.Sigmoid()\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutions\n",
    "        x = self.conv1_activation(self.conv1(x))\n",
    "        x = self.conv1_pooling(x)\n",
    "        # Flattern\n",
    "        x = self.flatten(x)\n",
    "        # Dense\n",
    "        x = self.linear1_activation(\n",
    "            self.linear1(x)\n",
    "        )\n",
    "        x = self.dense(x)\n",
    "        return self.to_probability(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80cd80-5a99-4456-bd7c-6be6d9a04d5a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f4bbc7be-bd13-4e58-a904-a629737662c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b4375676-88bd-4bb3-9890-ed354b81b993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:   0.000:   0%|                                                                                                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "Epoch Steps - Loss:   0.000:   0%|                                                                                                                                                    | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch Steps - Loss:  20.546:   0%|                                                                                                                                                    | 0/7 [00:03<?, ?it/s]\u001b[A\n",
      "Epoch Steps - Loss:  20.546:  14%|████████████████████                                                                                                                        | 1/7 [00:03<00:22,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.752:  14%|████████████████████                                                                                                                        | 1/7 [00:07<00:22,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.752:  29%|████████████████████████████████████████                                                                                                    | 2/7 [00:07<00:18,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  20.288:  29%|████████████████████████████████████████                                                                                                    | 2/7 [00:11<00:18,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  20.288:  43%|████████████████████████████████████████████████████████████                                                                                | 3/7 [00:11<00:14,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.569:  43%|████████████████████████████████████████████████████████████                                                                                | 3/7 [00:14<00:14,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.569:  57%|████████████████████████████████████████████████████████████████████████████████                                                            | 4/7 [00:14<00:11,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.444:  57%|████████████████████████████████████████████████████████████████████████████████                                                            | 4/7 [00:18<00:11,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.444:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 5/7 [00:18<00:07,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.659:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████                                        | 5/7 [00:22<00:07,  3.72s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.659:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 6/7 [00:22<00:03,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.708:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 6/7 [00:25<00:03,  3.73s/it]\u001b[A\n",
      "Epoch Steps - Loss:  21.708: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:25<00:00,  3.63s/it]\u001b[A\n",
      "Loss:  21.281: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:25<00:00, 25.38s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = tqdm( range(1), desc=f'Loss: {0.0:>7.3f}' )\n",
    "for i_epoch in epochs:\n",
    "    losses_epoch = []\n",
    "    steps = tqdm(dataloader, desc=f'Epoch Steps - Loss: {0.0:>7.3f}')\n",
    "    for x_train, y_train in steps:\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        y_train_pred = model(x_train)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_train_pred, y_train)\n",
    "        loss.backward()\n",
    "        loss_value = loss.cpu().item()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        losses_epoch.append(loss_value)\n",
    "        steps.set_description(\n",
    "            'Epoch Steps - Loss: {:>7.3f}'.format(loss_value)\n",
    "        )\n",
    "    logs.append([\n",
    "        np.mean(losses_epoch),\n",
    "        losses_epoch.copy()\n",
    "    ])\n",
    "    # Report\n",
    "    epochs.set_description(\n",
    "        'Loss: {:>7.3f}'.format( logs[-1][0] )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be83e2-5400-41a2-a109-25534dd0e7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c085c-4d9f-469a-8e1f-079e8d6f18bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6999dc5-00d0-47de-afdb-4d8f0cd0c90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e666891-6b19-4686-88cd-bd660bdbe16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328cd8f-bef4-454f-bff4-81ff58bec188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020a3de-a545-4a64-93ac-5512b0f1b437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c169b-3993-4f62-a4d0-ad311abb644f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efad11-631a-477b-a9eb-a34188425ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc31c8-3f4a-4d9c-9c52-39d9be54dc20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768bd26d-3768-435c-b73f-dcf184280dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5b5be-169f-4a68-905a-2f7eb0bfa045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90076d8-2da8-48b1-8c82-80aff1abbcf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76e7f4-d4e2-4f75-99fc-34a019d1be13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96add95-41bc-4df3-a761-e82ee5e89f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf10eb-9755-4577-a399-bb7f25a0a698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5aeb0-b925-4e8f-b799-8ca6c7c5de2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aafb11-572e-4aea-b884-5cc7a578d861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840a58e-7f62-498b-a619-cc55ae6cbc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d6543-ba4b-400b-9ae6-35f4d5aeeba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb7b24e-7933-4b40-9b00-434f7fbacded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0d02b-4fb7-430e-af6b-c7fe351c8287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5455389-e3fc-4ff0-87fc-ba68878be32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651166a-424a-4cd2-b50a-1970997ea6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6b73e-cbcd-488d-8a31-d27bc10c4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bed6708-010c-4928-bd35-4c5974c0d34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7413e49a-35f7-45ce-842a-1fbc7e25c6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82f273-2efb-4d5f-b4ae-0fccde3e7574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd896c63-3a36-489b-b8c1-4a9fa9fa35fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cfm]",
   "language": "python",
   "name": "conda-env-cfm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
